\section{Introduction}
В настоящее время нейросетевые модели являются передним краем науки (State-of-the-Art) в подавляющем большинстве задач машинного обучения.
Примерами могут быть задачи компьютерного зрения, обработки естественного языка и обучения на табличных данных.
Одним из важнейших преимуществ нейронных сетей является избавление от эвристик, присущих классическим алгоритмам обучения.
Это способствует большей предсказательной силе нейросетевых моделей при условии применения достаточного числа данных.
Однако большим минусом нейросетевых моделей является большая сложность обучения и связанная с этим неопределенность в выборе так называемых \textit{гиперпараметров}, которые значительно влияют на качество полученной модели.
В настоящее время наиболее распространённым способом для оптимизации гиперпараметров является ручной подбор~\cite{bouthillier2020survey}.
Однако из-за больших трудозатрат этот подход нельзя признать удовлетворительным.

В силу того, что гиперпараметры имеют совершенно разный характер, многие из них не допускают дифференцирование выхода сети, либо такие алгоритмы не реализованы, к оптимизации гиперпараметров приходится подходить как к ``чёрному ящику'' (\textit{black-box optimization}).
Это исключает градиентные и даже выпуклые методы для оптимизации, оставшиеся же заметно уступают в скорости.
Именно скорость является главной проблемой: один сеанс обучения нейросети может занимать часы, а запусков могут потребоваться десятки.
Поэтому большой интерес представляют методы для ускорения подбора гиперпараметров.

Одним из подходов здесь является ранняя остановка безперспективных исследований.
Для этого используются какие-либо промежуточные результаты.
Один из подходов к такой остановке предложен в данной работе.